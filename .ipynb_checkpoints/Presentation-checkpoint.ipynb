{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modul Spezielle Anwendungen der Informatik: \n",
    "# K.I. in der Robotik\n",
    "\n",
    "### Wintersemester 2018/2019 - HTW Berlin\n",
    "\n",
    "### Rekurrente neuronale Netze in PyTorch am Beispiel eines simplen LSTM-Maschinenübersetzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Framework imports\n",
    "import torch\n",
    "from torch import optim\n",
    "import os\n",
    "import random\n",
    "\n",
    "### Custom imports \n",
    "from model.model import *\n",
    "from experiment.train_eval import evaluateInput, GreedySearchDecoder, trainIters, eval_batch, plot_training_results\n",
    "from global_settings import device, FILENAME, SAVE_DIR, PREPRO_DIR, TRAIN_FILE, TEST_FILE, EXPERIMENT_DIR, LOG_FILE\n",
    "from model.model import EncoderLSTM, DecoderLSTM\n",
    "from utils.prepro import read_lines, preprocess_pipeline, load_cleaned_data, save_clean_data\n",
    "from utils.tokenize import build_vocab, batch2TrainData, indexesFromSentence\n",
    "\n",
    "from global_settings import DATA_DIR\n",
    "from utils.utils import split_data, filter_pairs, max_length, plot_grad_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translate import translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projekt\n",
    "- Rekurrente neuronale Netze, LSTMs\n",
    "- Anwendung in PyTorch\n",
    "- Implementierung eines naiven LSTM-Maschinenübersetzers, der in der Lage ist, kurze Sätze (\"how are you\" > \"wie geht es dir\") zu übersetzen\n",
    "- Übersetzung: EN > DE\n",
    "- Datensatz: [Tatoeba-Projekt](https://tatoeba.org/) (ca. 174000 Sprachpaare)\n",
    "    - Reduzierung auf ca. 159.000 (Satzlänge 10)\n",
    "    - Sprachdomäne: Alltag --> Ganz einfache Sätze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das Ergebnis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_EXPERIMENT = \"experiment/checkpoints/dry_run_simple_nmt_model_full_158544_teacher_1.0_train_voc_adam_lr-0.001-1/deu.txt/2-2_512-512_100\"\n",
    "SECOND_BEST_EXPERIMENT = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projektpresäntation...\n",
      "Reading experiment information from: \n",
      "Starting translation process...\n",
      "Source > hi\n",
      "Translation >  hallo\n",
      "Source > how are you\n",
      "Translation >  wie gehts dir\n",
      "Source > where are you\n",
      "Translation >  wo sind sie\n",
      "Source > I love you\n",
      "Translation >  ich liebe dich\n",
      "Source > I am in the school\n",
      "Translation >  ich bin in der schule\n",
      "Source > I love music\n",
      "Translation >  ich liebe musik\n",
      "Source > The man at the bus stop is my uncle\n",
      "Translation >  der der der mann in den onkel mein onkel\n",
      "Source > I think you should stop speaking\n",
      "Translation >  ich denke du sollten aufhoeren zu reden\n",
      "Source > the taxi is here\n",
      "Translation >  das taxi ist hier\n",
      "Source > the train is here\n",
      "Translation >  der zug ist hier\n",
      "Source > the train has arrived\n",
      "Translation >  der zug ist da\n",
      "Source > the train has just arrived\n",
      "Translation >  der zug ist gerade angekommen\n",
      "Source > quit\n"
     ]
    }
   ],
   "source": [
    "print(\"Projektpresäntation...\")\n",
    "translate(start_root=\".\", path=BEST_EXPERIMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modellarchitektur\n",
    "\n",
    "### Sequence-to-Sequence (Seq2Seq) Modell oder Encoder-Decoder-Architektur\n",
    "\n",
    "- Bezeichnung für einen neuronalen Maschinenübersetzer\n",
    "- Verarbeitung von Sequenzen: Eingaben und Ausgaben sind Sequenzen, deren Länge im Voraus nicht bekannt ist. Diese Länge muss auch nicht übereinstimmen\n",
    "\n",
    "<img src=\"documentation/seq2seq.png\" alt=\"Seq2Seq Models\" width=600>\n",
    "\n",
    "Quelle: https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-2-seq2seq-model-encoderdecoder-model-6c22e29fd7e1\n",
    "\n",
    "- **S** ist der sogenannte Kontextvektor, eine Zusammenfassung der verarbeiteten Sequenz\n",
    "    - Sequenzen liegen als Zahlen vor und das Modell soll aus ihnen lernen\n",
    "    - Insbesondere muss das Modell die \"Semantik\" lernen\n",
    "        - Große Schwierigkeiten (!)\n",
    "        - Ferne Zusammenhänge: **Der Mann**, der an der Haltestelle steht, **ist** mein Onkel --> Zwischen Subjekt und Verb stehen 5 Wörter! Für Menschen ist das easy, für Maschinen etwas weniger\n",
    "\n",
    "### Das Geheimnis ist das Gedächtnis --> Rekurrente neuronale Netze (RNNs)\n",
    "\n",
    "- Informationen \"persistieren\"\n",
    "- Eingaben/Ausgaben variabler Länge verarbeiten\n",
    "- Parameter werden durch die ganze Sequenz geteilt --> Sequenz (z.B. maximale Satzlänge in einer Batch) == Schritte == Zeit\n",
    "    - Parameter:\n",
    "        - U --> unter Eingaben und hidden layers geteilt\n",
    "        - W --> unter den hidden layers geteilt\n",
    "        - V --> unter hidden layers und Ausgaben geteilt\n",
    "\n",
    "<img src=\"documentation/rnn.jpg\" alt=\"Seq2Seq Models\" width=600>\n",
    "\n",
    "Quelle: http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "\n",
    "Bezeichnungen:\n",
    "- $x_{t}$ - Input beim Schritt $t$, z.B. das zweite Wort\n",
    "- $s_{t}$ ist der Hidden-State zum Schritt $t$. Das ist der Speicher vom ganzen Netz: $s_{t} = \\text{f}(Ux_t + Ws_{t-1})$ (f = tanh oder ReLU) --> **$s_{0}$ ist in der Regel mit 0 initialisiert**\n",
    "- $o_{t}$ ist der Ausgabe == Wahrscheinlichkeitsverteilung über alle möglichen Klassen, daher: $o_{t} = softmax(Vs_{t})$\n",
    "\n",
    "---> \"Short Memory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMs (Long Short-Term Memory)\n",
    "\n",
    "- Hidden State (h oder s) + Cell State (c) (Memory)\n",
    "\n",
    "<img src=\"documentation/LSTM3-chain.png\" alt=\"Seq2Seq Models\" width=600>\n",
    "Quelle: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "Jede Zelle im Bild ist ein Neuron im ganzen Netz.\n",
    "\n",
    "**Gating-Mechanismus**:\n",
    "\n",
    "<img src=\"documentation/lstm_medium.png\" alt=\"Seq2Seq Models\" height=500 width=650>\n",
    "Quelle: https://medium.com/@saurabh.rathor092/simple-rnn-vs-gru-vs-lstm-difference-lies-in-more-flexible-control-5f33e07b1e57\n",
    "\n",
    "- **Forget**-Gate --> Wie viel vom \"Gedächtnis\" muss behalten werden? $f_t$\n",
    "- **Input**-Gate -->  $ i_t$\n",
    "- Berechnung der C-Kandidaten: $\\tilde C_{t}$\n",
    "- **Output-Gate**: $o_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mehrschichtiges Netz \n",
    "\n",
    "<img src=\"documentation/trados_nmt.png\">\n",
    "Source: https://www.sdl.com/ilp/language/neural-machine-translation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "1. Vocabularies erstellen\n",
    "2. Wörter darstellen --> Embeddings\n",
    "3. Batch generieren --> (seq_len, batch_size)\n",
    "4. Training:\n",
    "    - Batch wird vom Encoder (als Batch) verarbeitet. Encoder liefert h\n",
    "    - Decoder wird durch SOS initialisiert + Target-Batch + encoder hidden state (als erstes hidden state)\n",
    "    - Bei jedem Schritt wird ein Wort vorhergesagt + hidden state aktualisiert\n",
    "5. Die Übersetzung besteht aus den Wörtern mit der höchsten Wahrscheinlichkeit \n",
    "\n",
    "#### Einige Maßnahmen:\n",
    "- Backpropagation Through Time --> Rekursion wird berücksichtigt, daher \"Through Time\"\n",
    "- Teacher Forcing gegen langsame Konvergenz bzw. schlechte Leistungen\n",
    "- Gradient Clipping --> kein Vanishing oder Exploding Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embeddings\n",
    "\n",
    "$q_\\text{mathematician} = \\left[ \\overbrace{2.3}^\\text{can run},\n",
    "\\overbrace{9.4}^\\text{likes coffee}, \\overbrace{-5.5}^\\text{majored in Physics}, \\dots \\right]$\n",
    "\n",
    "$q_\\text{physicist} = \\left[ \\overbrace{2.5}^\\text{can run},\n",
    "\\overbrace{9.1}^\\text{likes coffee}, \\overbrace{6.4}^\\text{majored in Physics}, \\dots \\right]$\n",
    "\n",
    "\n",
    "Quelle: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching Example\n",
    "\n",
    "\"I went to school\" (25, 858, 48, 284)\n",
    "\n",
    "<img src=\"documentation/seq2seq_batches.png\">\n",
    "Quelle: https://pytorch.org/tutorials/beginner/chatbot_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quellen\n",
    "\n",
    "### Praxis: \n",
    "\n",
    "- Chatbot-Tutorial (PyTorch): https://pytorch.org/tutorials/beginner/chatbot_tutorial.html\n",
    "- Sequence-to-Sequence Tutorial (PyTorch): https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "- Machine Learning Mastery \"How to develop a neural machine translator from scratch\": https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/\n",
    "\n",
    "### Theorie (eine Auswahl):\n",
    "- Sutskever et al. (2014), \"Sequence to Sequence Learning with Neural Networks\": https://arxiv.org/abs/1409.3215\n",
    "- Stanford NLP (CS224N): http://web.stanford.edu/class/cs224n/index.html#schedule\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
