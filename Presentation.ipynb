{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modul Spezielle Anwendungen der Informatik: K.I. in der Robotik\n",
    "\n",
    "## Projektpräsentation: Sequenzmodelle in PyTorch am Beispiel eines simplen LSTM-Maschinenübersetzers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modellarchitektur\n",
    "\n",
    "### Sequence-to-Sequence Modell\n",
    "\n",
    "<img src=\"documentation/seq2seq.png\" alt=\"Seq2Seq Models\" width=800>\n",
    "\n",
    "Quelle: https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-2-seq2seq-model-encoderdecoder-model-6c22e29fd7e1\n",
    "\n",
    "\n",
    "### Verarbeitung: RNN\n",
    "<img src=\"documentation/RNN-unrolled.png\" alt=\"Seq2Seq Models\" width=600 height=400>\n",
    "\n",
    "Quelle: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Framework imports\n",
    "import torch\n",
    "from torch import optim\n",
    "import os\n",
    "import random\n",
    "\n",
    "### Custom imports \n",
    "from model.model import *\n",
    "from experiment.train_eval import evaluateInput, GreedySearchDecoder, trainIters, eval_batch, plot_training_results\n",
    "from global_settings import device, FILENAME, SAVE_DIR, PREPRO_DIR, TRAIN_FILE, TEST_FILE, EXPERIMENT_DIR, LOG_FILE\n",
    "from model.model import EncoderLSTM, DecoderLSTM\n",
    "from utils.prepro import read_lines, preprocess_pipeline, load_cleaned_data, save_clean_data\n",
    "from utils.tokenize import build_vocab, batch2TrainData, indexesFromSentence\n",
    "\n",
    "from global_settings import DATA_DIR\n",
    "from utils.utils import split_data, filter_pairs, max_length, plot_grad_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists. Loading cleaned pairs...\n"
     ]
    }
   ],
   "source": [
    "### Data cleaning\n",
    "start_root = \".\"\n",
    "exp_contraction = True # don't --> do not\n",
    "file_to_load = \"simple_dataset_praesi.txt\"\n",
    "file_name = \"simple_dataset_praesi.pkl\"\n",
    "\n",
    "\n",
    "if os.path.isfile(os.path.join(start_root, PREPRO_DIR,file_name)):\n",
    "    ##load\n",
    "    print(\"File exists. Loading cleaned pairs...\")\n",
    "    pairs = load_cleaned_data(PREPRO_DIR, filename=file_name)\n",
    "else: \n",
    "    print(\"Preprocessing file...\")\n",
    "    ### read lines from file\n",
    "    pairs = read_lines(os.path.join(start_root,DATA_DIR),file_to_load)\n",
    "    ### Preprocess file\n",
    "    pairs, path = preprocess_pipeline(pairs, file_name, exp_contraction, max_len = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i fell', 'ich fiel']\n",
      "Total pairs in the small dataset:\n",
      "100\n",
      "Max length in source sentences: [3]\n",
      "Max length in target sentences: [5]\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(pairs))\n",
    "print(\"Total pairs in the small dataset:\")\n",
    "print(len(pairs))\n",
    "\n",
    "train_pairs = pairs\n",
    "src_sents = [pair[0] for pair in pairs]\n",
    "trg_sents = [pair[1] for pair in pairs]\n",
    "\n",
    "max_src_l = max_length(src_sents)\n",
    "max_trg_l = max_length(trg_sents)\n",
    "\n",
    "print(\"Max length in source sentences:\", max_src_l)\n",
    "print(\"Max length in target sentences:\", max_trg_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get out\n",
      "verkruemele dich\n"
     ]
    }
   ],
   "source": [
    "### Getting src and trg sents\n",
    "src_sents, trg_sents = [], []\n",
    "src_sents = [item[0] for item in pairs]\n",
    "trg_sents = [item[1] for item in pairs]\n",
    "print(random.choice(src_sents))\n",
    "print(random.choice(trg_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total source words: 55\n",
      "Total target words: 125\n",
      "\n",
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'hi': 4, 'run': 5, 'wow': 6, 'fire': 7, 'help': 8, 'stop': 9, 'wait': 10, 'go': 11, 'on': 12, 'hello': 13, 'i': 14, 'ran': 15, 'see': 16, 'try': 17, 'won': 18, 'smile': 19, 'cheers': 20, 'freeze': 21, 'got': 22, 'it': 23, 'he': 24, 'hop': 25, 'in': 26, 'hug': 27, 'me': 28, 'fell': 29, 'know': 30, 'lied': 31, 'lost': 32, 'paid': 33, 'swim': 34, 'am': 35, 'ok': 36, 'up': 37, 'no': 38, 'way': 39, 'really': 40, 'thanks': 41, 'why': 42, 'ask': 43, 'tom': 44, 'be': 45, 'cool': 46, 'fair': 47, 'nice': 48, 'beat': 49, 'call': 50, 'come': 51, 'get': 52, 'out': 53, 'away': 54}\n",
      "\n",
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, 'hallo': 4, 'gruess': 5, 'gott': 6, 'lauf': 7, 'potzdonner': 8, 'donnerwetter': 9, 'feuer': 10, 'hilfe': 11, 'zu': 12, 'huelf': 13, 'stopp': 14, 'warte': 15, 'mach': 16, 'weiter': 17, 'ich': 18, 'rannte': 19, 'verstehe': 20, 'aha': 21, 'probiere': 22, 'es': 23, 'hab': 24, 'gewonnen': 25, 'habe': 26, 'laecheln': 27, 'zum': 28, 'wohl': 29, 'keine': 30, 'bewegung': 31, 'stehenbleiben': 32, 'kapiert': 33, 'verstanden': 34, 'einverstanden': 35, 'er': 36, 'lief': 37, 'mit': 38, 'drueck': 39, 'mich': 40, 'nimm': 41, 'in': 42, 'den': 43, 'arm': 44, 'umarme': 45, 'fiel': 46, 'hin': 47, 'stuerzte': 48, 'bin': 49, 'hingefallen': 50, 'gestuerzt': 51, 'weiss': 52, 'gelogen': 53, 'verloren': 54, 'bezahlt': 55, 'zahlte': 56, 'schwimme': 57, 'jahre': 58, 'alt': 59, 'mir': 60, 'gehts': 61, 'gut': 62, 'geht': 63, 'wach': 64, 'auf': 65, 'unmoeglich': 66, 'das': 67, 'kommt': 68, 'nicht': 69, 'frage': 70, 'gibts': 71, 'doch': 72, 'ausgeschlossen': 73, 'keinster': 74, 'weise': 75, 'wirklich': 76, 'echt': 77, 'im': 78, 'ernst': 79, 'danke': 80, 'versuchs': 81, 'warum': 82, 'frag': 83, 'tom': 84, 'fragen': 85, 'sie': 86, 'fragt': 87, 'entspann': 88, 'dich': 89, 'sei': 90, 'ungerecht': 91, 'fair': 92, 'nett': 93, 'seien': 94, 'geh': 95, 'weg': 96, 'hau': 97, 'ab': 98, 'verschwinde': 99, 'verdufte': 100, 'fort': 101, 'zieh': 102, 'leine': 103, 'vom': 104, 'acker': 105, 'verzieh': 106, 'verkruemele': 107, 'troll': 108, 'zisch': 109, 'pack': 110, 'ne': 111, 'fliege': 112, 'schwirr': 113, 'die': 114, 'sause': 115, 'scher': 116, 'ruf': 117, 'an': 118, 'komm': 119, 'herein': 120, 'schon': 121, 'macht': 122, 'hol': 123, 'raus': 124}\n",
      "\n",
      "Example of conversion word > index:\n",
      "Word hello > Index 13\n",
      "Index 20 > Word cheers\n"
     ]
    }
   ],
   "source": [
    "### Creating vocabularies\n",
    "input_lang = build_vocab(src_sents, \"eng\")\n",
    "output_lang = build_vocab(trg_sents, \"deu\")\n",
    "\n",
    "print(\"Total source words:\", input_lang.num_words)\n",
    "print(\"Total target words:\", output_lang.num_words)\n",
    "print(\"\")\n",
    "print(input_lang.word2index)\n",
    "print(\"\")\n",
    "print(output_lang.word2index)\n",
    "print(\"\")\n",
    "print(\"Example of conversion word > index:\")\n",
    "print(\"Word {} > Index {}\".format('hello', input_lang.word2index.get('hello')))\n",
    "print(\"Index {} > Word {}\".format(20, input_lang.index2word.get(20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i paid', 'ich zahlte']\n"
     ]
    }
   ],
   "source": [
    "### Simple conversion sentence to tensor:\n",
    "random_pair = train_pairs[40]\n",
    "print(random_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 33, 2]\n",
      "[18, 56, 2]\n"
     ]
    }
   ],
   "source": [
    "english_sent = indexesFromSentence(input_lang, random_pair[0])\n",
    "german_sent = indexesFromSentence(output_lang, random_pair[1])\n",
    "\n",
    "print(english_sent)\n",
    "print(german_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: got it Target: verstanden\n",
      "Src tensor: [22, 23, 2] Trg tensor: [34, 2]\n",
      "Source: got it Target: einverstanden\n",
      "Src tensor: [22, 23, 2] Trg tensor: [35, 2]\n",
      "Source: i see Target: ich verstehe\n",
      "Src tensor: [14, 16, 2] Trg tensor: [18, 20, 2]\n",
      "Source: i am Target: ich bin jahre alt\n",
      "Src tensor: [14, 35, 2] Trg tensor: [18, 49, 58, 59, 2]\n",
      "Source: really Target: im ernst\n",
      "Src tensor: [40, 2] Trg tensor: [78, 79, 2]\n"
     ]
    }
   ],
   "source": [
    "### No splitting for this short presentation :-)\n",
    "train_pairs = pairs\n",
    "mini_batch = 5\n",
    "batch_pair = [random.choice(train_pairs) for _ in range(5)]\n",
    "batch_pair.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "for pair in batch_pair:\n",
    "    print(\"Source:\", pair[0],\"Target:\", pair[1])    \n",
    "    print(\"Src tensor:\", indexesFromSentence(input_lang, pair[0]),\"Trg tensor:\", indexesFromSentence(output_lang, pair[1]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a simple batch of 5 sentences --> Shape (seq_len, batch_size)\n",
    "training_batch = batch2TrainData(input_lang, output_lang, batch_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, input_lengths, target_tensor, mask, target_max_len, target_lengths = training_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of source sentences: tensor([3, 3, 3, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of source sentences:\", input_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorized input:\n",
      "tensor([[22, 22, 14, 14, 40],\n",
      "        [23, 23, 16, 35,  2],\n",
      "        [ 2,  2,  2,  2,  0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorized input:\")\n",
    "print(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorized output:\n",
      "tensor([[34, 35, 18, 18, 78],\n",
      "        [ 2,  2, 20, 49, 79],\n",
      "        [ 0,  0,  2, 58,  2],\n",
      "        [ 0,  0,  0, 59,  0],\n",
      "        [ 0,  0,  0,  2,  0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorized output:\")\n",
    "print(target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoding - Decoding Verfahren\n",
    "\n",
    "\n",
    "Sowohl Encoder als auch Decoder greifen auf das erste Index zu, sprich die Eingaben nicht über die batch_size Dimension verarbeitet, sondern als Sequenz nach ihrer Sequenzlänge verarbeitet, wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 0\n",
      "Input: tensor([22, 22, 14, 14, 40])\n",
      "Woerter: ['got', 'got', 'i', 'i', 'really']\n",
      "Timestep: 1\n",
      "Input: tensor([23, 23, 16, 35,  2])\n",
      "Woerter: ['it', 'it', 'see', 'am', '<EOS>']\n",
      "Timestep: 2\n",
      "Input: tensor([2, 2, 2, 2, 0])\n",
      "Woerter: ['<EOS>', '<EOS>', '<EOS>', '<EOS>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "### Das bekommt das Encoder bzw. Decoder zu jedem Zeitschritt t:\n",
    "for i, elem in enumerate(input_tensor):\n",
    "    print(\"Timestep:\", i)\n",
    "    print(\"Input:\", elem)\n",
    "    print(\"Woerter:\", [input_lang.index2word[word.item()] for word in elem])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 0\n",
      "Input: tensor([34, 35, 18, 18, 78])\n",
      "Woerter: ['verstanden', 'einverstanden', 'ich', 'ich', 'im']\n",
      "Timestep: 1\n",
      "Input: tensor([ 2,  2, 20, 49, 79])\n",
      "Woerter: ['<EOS>', '<EOS>', 'verstehe', 'bin', 'ernst']\n",
      "Timestep: 2\n",
      "Input: tensor([ 0,  0,  2, 58,  2])\n",
      "Woerter: ['<PAD>', '<PAD>', '<EOS>', 'jahre', '<EOS>']\n",
      "Timestep: 3\n",
      "Input: tensor([ 0,  0,  0, 59,  0])\n",
      "Woerter: ['<PAD>', '<PAD>', '<PAD>', 'alt', '<PAD>']\n",
      "Timestep: 4\n",
      "Input: tensor([0, 0, 0, 2, 0])\n",
      "Woerter: ['<PAD>', '<PAD>', '<PAD>', '<EOS>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "### Genauso im Decoder\n",
    "### Das bekommt das Encoder bzw. Decoder zu jedem Zeitschritt t:\n",
    "for i, elem in enumerate(target_tensor):\n",
    "    print(\"Timestep:\", i)\n",
    "    print(\"Input:\", elem)\n",
    "    print(\"Woerter:\", [output_lang.index2word[word.item()] for word in elem])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTMs \n",
    "\n",
    "<img src=\"documentation/LSTM3-chain.png\" alt=\"Seq2Seq Models\" width=800>\n",
    "Quelle: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "Gating-Mechanismus:\n",
    "- Forget-Gate\n",
    "- Input-Gate\n",
    "- Berechnung der C-Kandidaten\n",
    "- Output-Gate\n",
    "\n",
    "Berechnung des finalen hidden state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Übersetzen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translate import translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_EXPERIMENT = \"experiment/checkpoints/dry_run_simple_nmt_model_full_158544_teacher_1.0_train_voc_adam_lr-0.001-1/deu.txt/2-2_512-512_100\"\n",
    "SECOND_BEST_EXPERIMENT = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading experiment information from: \n",
      "Starting translation process...\n",
      "Source > hi\n",
      "Translation >  hallo\n",
      "Source > how are you\n",
      "Translation >  wie gehts dir\n",
      "Source > I am fine\n",
      "Translation >  es geht mir gut\n",
      "Source > How old are you\n",
      "Translation >  wie alt sind sie\n",
      "Source > WHere are you\n",
      "Translation >  wo sind sie\n",
      "Source > I live in Germany\n",
      "Translation >  ich lebe in deutschland\n",
      "Source > I live near you\n",
      "Translation >  ich wohne in der naehe\n",
      "Source > I want to go to the supermarket\n",
      "Translation >  ich will zum supermarkt gehen\n",
      "Source > I think you are amazing\n",
      "Translation >  ich finde du bist toll\n",
      "Source > I am amazing\n",
      "Translation >  ich bin unglaublich\n",
      "Source > I need some holiday\n",
      "Translation >  ich brauche etwas urlaub\n",
      "Source > I need some cake\n",
      "Translation >  ich brauche etwas kuchen\n",
      "Source > I need sweeties\n",
      "Translation >  ich brauche politisches\n",
      "Source > I think you should stop screaming\n",
      "Translation >  ich finde du sollten aufhoeren zu schreien\n",
      "Source > i think you should stop writing this presentation\n",
      "Translation >  ich denke du sollten dieses diesem zu zu schreiben\n",
      "Source > I think you are interesting\n",
      "Translation >  ich finde du interessant\n",
      "Source > I want to go to bed\n",
      "Translation >  ich will ins bett gehen\n",
      "Source > Good night\n",
      "Translation >  gute nacht\n",
      "Source > Sleep good\n",
      "Translation >  schlafe gut\n",
      "Source > We see us tomorrow\n",
      "Translation >  wir sehen uns morgen\n",
      "Source > Is it okay?\n",
      "Translation >  ist es gut\n",
      "Source > Lets say at twelve\n",
      "Translation >  sagen sie am am am\n",
      "Source > Okay, now I will leave\n",
      "Translation >  jetzt wo werde ich jetzt\n",
      "Source > I am leaving\n",
      "Translation >  ich gehe\n",
      "Source > The train has just arrived\n",
      "Translation >  der zug ist gerade angekommen\n",
      "Source > The taxi is here\n",
      "Translation >  das taxi ist hier\n",
      "Source > I must go\n",
      "Translation >  ich muss gehen\n",
      "Source > You will come late\n",
      "Translation >  du kommst kommen kommen\n",
      "Source > Hurry up\n",
      "Translation >  beeil dich\n",
      "Source > Goodbye\n",
      "Translation >  tschuess\n",
      "Source > q\n"
     ]
    }
   ],
   "source": [
    "translate(start_root=\".\", path=BEST_EXPERIMENT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
