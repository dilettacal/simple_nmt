{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modul Spezielle Anwendungen der Informatik: K.I. in der Robotik\n",
    "\n",
    "## Projektpräsentation: Sequenzmodelle in PyTorch am Beispiel eines simplen LSTM-Maschinenübersetzers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Modellarchitektur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Framework imports\n",
    "import torch\n",
    "from torch import optim\n",
    "import os\n",
    "import random\n",
    "\n",
    "### Custom imports \n",
    "from model.model import *\n",
    "from experiment.train_eval import evaluateInput, GreedySearchDecoder, trainIters, eval_batch, plot_training_results\n",
    "from global_settings import device, FILENAME, SAVE_DIR, PREPRO_DIR, TRAIN_FILE, TEST_FILE, EXPERIMENT_DIR, LOG_FILE\n",
    "from model.model import EncoderLSTM, DecoderLSTM\n",
    "from utils.prepro import read_lines, preprocess_pipeline, load_cleaned_data, save_clean_data\n",
    "from utils.tokenize import build_vocab, batch2TrainData, indexesFromSentence\n",
    "\n",
    "from global_settings import DATA_DIR\n",
    "from utils.utils import split_data, filter_pairs, max_length, plot_grad_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists. Loading cleaned pairs...\n"
     ]
    }
   ],
   "source": [
    "### Data cleaning\n",
    "start_root = \".\"\n",
    "exp_contraction = True # don't --> do not\n",
    "file_to_load = \"simple_dataset_praesi.txt\"\n",
    "file_name = \"simple_dataset_praesi.pkl\"\n",
    "\n",
    "\n",
    "if os.path.isfile(os.path.join(start_root, PREPRO_DIR,file_name)):\n",
    "    ##load\n",
    "    print(\"File exists. Loading cleaned pairs...\")\n",
    "    pairs = load_cleaned_data(PREPRO_DIR, filename=cleaned_file)\n",
    "else: \n",
    "    print(\"Preprocessing file...\")\n",
    "    ### read lines from file\n",
    "    pairs = read_lines(os.path.join(start_root,DATA_DIR),file_to_load)\n",
    "    ### Preprocess file\n",
    "    pairs, path = preprocess_pipeline(pairs, file_name, exp_contraction, max_len = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stop', 'stopp']\n",
      "Total pairs in the small dataset:\n",
      "100\n",
      "Max length in source sentences: [3]\n",
      "Max length in target sentences: [5]\n"
     ]
    }
   ],
   "source": [
    "print(random.choice(pairs))\n",
    "print(\"Total pairs in the small dataset:\")\n",
    "print(len(pairs))\n",
    "\n",
    "max_src_l = max_length(src_sents)\n",
    "max_trg_l = max_length(trg_sents)\n",
    "\n",
    "print(\"Max length in source sentences:\", max_src_l)\n",
    "print(\"Max length in target sentences:\", max_trg_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beat it\n",
      "macht schon\n"
     ]
    }
   ],
   "source": [
    "### Getting src and trg sents\n",
    "src_sents, trg_sents = [], []\n",
    "src_sents = [item[0] for item in pairs]\n",
    "trg_sents = [item[1] for item in pairs]\n",
    "print(random.choice(src_sents))\n",
    "print(random.choice(trg_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total source words: 55\n",
      "Total target words: 125\n",
      "Example of conversion word > index:\n",
      "Word hello > Index 13\n",
      "Index 20 > Word cheers\n"
     ]
    }
   ],
   "source": [
    "### Creating vocabularies\n",
    "input_lang = build_vocab(src_sents, \"eng\")\n",
    "output_lang = build_vocab(trg_sents, \"deu\")\n",
    "\n",
    "print(\"Total source words:\", input_lang.num_words)\n",
    "print(\"Total target words:\", output_lang.num_words)\n",
    "\n",
    "print(\"Example of conversion word > index:\")\n",
    "print(\"Word {} > Index {}\".format('hello', input_lang.word2index.get('hello')))\n",
    "print(\"Index {} > Word {}\".format(20, input_lang.index2word.get(20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i paid', 'ich zahlte']\n"
     ]
    }
   ],
   "source": [
    "### Simple conversion sentence to tensor:\n",
    "random_pair = train_pairs[40]\n",
    "print(random_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 33, 2]\n",
      "[18, 56, 2]\n"
     ]
    }
   ],
   "source": [
    "english_sent = indexesFromSentence(input_lang, random_pair[0])\n",
    "german_sent = indexesFromSentence(output_lang, random_pair[1])\n",
    "\n",
    "print(english_sent)\n",
    "print(german_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: beat it Target: schwirr ab\n",
      "Src tensor: [49, 23, 2] Trg tensor: [113, 98, 2]\n",
      "Source: get tom Target: hol tom\n",
      "Src tensor: [52, 44, 2] Trg tensor: [123, 84, 2]\n",
      "Source: beat it Target: scher dich weg\n",
      "Src tensor: [49, 23, 2] Trg tensor: [116, 89, 96, 2]\n",
      "Source: really Target: echt\n",
      "Src tensor: [40, 2] Trg tensor: [77, 2]\n",
      "Source: wow Target: donnerwetter\n",
      "Src tensor: [6, 2] Trg tensor: [9, 2]\n"
     ]
    }
   ],
   "source": [
    "### No splitting for this short presentation :-)\n",
    "train_pairs = pairs\n",
    "mini_batch = 5\n",
    "batch_pair = [random.choice(train_pairs) for _ in range(5)]\n",
    "batch_pair.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "for pair in batch_pair:\n",
    "    print(\"Source:\", pair[0],\"Target:\", pair[1])    \n",
    "    print(\"Src tensor:\", indexesFromSentence(input_lang, pair[0]),\"Trg tensor:\", indexesFromSentence(output_lang, pair[1]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a simple batch of 5 sentences --> Shape (seq_len, batch_size)\n",
    "training_batch = batch2TrainData(input_lang, output_lang, batch_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, input_lengths, target_tensor, mask, target_max_len, target_lengths = training_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of source sentences: tensor([3, 3, 3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of source sentences:\", input_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorized input:\n",
      "tensor([[49, 52, 49, 40,  6],\n",
      "        [23, 44, 23,  2,  2],\n",
      "        [ 2,  2,  2,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorized input:\")\n",
    "print(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorized output:\n",
      "tensor([[113, 123, 116,  77,   9],\n",
      "        [ 98,  84,  89,   2,   2],\n",
      "        [  2,   2,  96,   0,   0],\n",
      "        [  0,   0,   2,   0,   0]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Tensorized output:\")\n",
    "print(target_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoding - Decoding Verfahren:\n",
    "\n",
    "Sowohl Encoder als auch Decoder greifen auf das erste Index zu, sprich die Eingaben nicht über die batch_size Dimension verarbeitet, sondern als Sequenz nach ihrer Sequenzlänge verarbeitet, wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 0\n",
      "Input: tensor([49, 52, 49, 40,  6])\n",
      "Woerter: ['beat', 'get', 'beat', 'really', 'wow']\n",
      "Timestep: 1\n",
      "Input: tensor([23, 44, 23,  2,  2])\n",
      "Woerter: ['it', 'tom', 'it', '<EOS>', '<EOS>']\n",
      "Timestep: 2\n",
      "Input: tensor([2, 2, 2, 0, 0])\n",
      "Woerter: ['<EOS>', '<EOS>', '<EOS>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "### Das bekommt das Encoder bzw. Decoder zu jedem Zeitschritt t:\n",
    "for i, elem in enumerate(input_tensor):\n",
    "    print(\"Timestep:\", i)\n",
    "    print(\"Input:\", elem)\n",
    "    print(\"Woerter:\", [input_lang.index2word[word.item()] for word in elem])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Genauso im Decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Übersetzen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
