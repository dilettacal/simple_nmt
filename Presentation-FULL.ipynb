{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modul Spezielle Anwendungen der Informatik: \n",
    "# K.I. in der Robotik\n",
    "\n",
    "### Wintersemester 2018/2019 - HTW Berlin\n",
    "\n",
    "### Rekurrente neuronale Netze in PyTorch am Beispiel eines simplen LSTM-Maschinenübersetzers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Framework imports\n",
    "import torch\n",
    "from torch import optim\n",
    "import os\n",
    "import random\n",
    "\n",
    "### Custom imports \n",
    "from model.model import *\n",
    "from experiment.train_eval import evaluateInput, GreedySearchDecoder, trainIters, eval_batch, plot_training_results\n",
    "from global_settings import device, FILENAME, SAVE_DIR, PREPRO_DIR, TRAIN_FILE, TEST_FILE, EXPERIMENT_DIR, LOG_FILE\n",
    "from model.model import EncoderLSTM, DecoderLSTM\n",
    "from utils.prepro import read_lines, preprocess_pipeline, load_cleaned_data, save_clean_data\n",
    "from utils.tokenize import build_vocab, batch2TrainData, indexesFromSentence\n",
    "\n",
    "from global_settings import DATA_DIR\n",
    "from utils.utils import split_data, filter_pairs, max_length, plot_grad_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from translate import translate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projekt\n",
    "- Rekurrente neuronale Netze, LSTMs\n",
    "- Anwendung in PyTorch\n",
    "- Implementierung eines naiven LSTM-Maschinenübersetzers, der in der Lage ist, kurze Sätze (\"how are you\" > \"wie geht es dir\") zu übersetzen\n",
    "- Übersetzung: EN > DE\n",
    "- Datensatz: [Tatoeba-Projekt](https://tatoeba.org/) (ca. 174000 Sprachpaare)\n",
    "    - Reduzierung auf ca. 159.000 (Satzlänge 10)\n",
    "    - Sprachdomäne: Alltag --> Ganz einfache Sätze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das Ergebnis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_EXPERIMENT = \"experiment/checkpoints/dry_run_simple_nmt_model_full_158544_teacher_1.0_train_voc_adam_lr-0.001-1/deu.txt/2-2_512-512_100\"\n",
    "SECOND_BEST_EXPERIMENT = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projektpresäntation...\n",
      "Reading experiment information from: \n",
      "Starting translation process...\n",
      "Source > hi\n",
      "Translation >  hallo\n",
      "Source > how are you\n",
      "Translation >  wie gehts dir\n",
      "Source > i am fine\n",
      "Translation >  es geht mir gut\n",
      "Source > i am in the university\n",
      "Translation >  ich bin in der universitaet\n",
      "Source > I am sad\n",
      "Translation >  ich bin traurig\n",
      "Source > I am happy\n",
      "Translation >  ich bin gluecklich\n",
      "Source > I love pizza\n",
      "Translation >  ich liebe pizza\n",
      "Source > I really enjoy films\n",
      "Translation >  mir gefaellt mir sehr gerne\n",
      "Source > I really enjoy football\n",
      "Translation >  mir gefaellt mir sehr gerne football\n",
      "Source > I like reading books\n",
      "Translation >  ich lese gerne buecher\n",
      "Source > I am in the kitchen with my mother\n",
      "Translation >  ich bin in der kueche meiner mutter\n",
      "Source > I am smart\n",
      "Translation >  ich bin schlau\n",
      "Source > I can translate short sentences\n",
      "Translation >  ich kann das saetze uebersetzen\n",
      "Source > the train has already gone\n",
      "Translation >  der zug ist schon weg\n",
      "Source > I have missed the bus\n",
      "Translation >  ich habe den bus verpasst\n",
      "Source > I love germany\n",
      "Translation >  ich liebe deutschland\n",
      "Source > I live in german\n",
      "Translation >  ich lebe in einer deutschen\n",
      "Source > I want to go abroad\n",
      "Translation >  ich will ins ausland gehen gehen\n",
      "Source > I think you should stop speaking\n",
      "Translation >  ich denke du sollten aufhoeren zu reden\n",
      "Source > the man at the station is my father\n",
      "Translation >  der mann der meinem vater ist mein vater\n"
     ]
    }
   ],
   "source": [
    "print(\"Projektpresäntation...\")\n",
    "translate(start_root=\".\", path=BEST_EXPERIMENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modellarchitektur\n",
    "\n",
    "### Sequence-to-Sequence (Seq2Seq) Modell oder Encoder-Decoder-Architektur\n",
    "\n",
    "- Bezeichnung für einen neuronalen Maschinenübersetzer\n",
    "- Verarbeitung von Sequenzen: Eingaben und Ausgaben sind Sequenzen, deren Länge im Voraus nicht bekannt ist. Diese Länge muss auch nicht übereinstimmen\n",
    "\n",
    "<img src=\"documentation/seq2seq.png\" alt=\"Seq2Seq Models\" width=600>\n",
    "\n",
    "Quelle: https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-2-seq2seq-model-encoderdecoder-model-6c22e29fd7e1\n",
    "\n",
    "- **S** ist der sogenannte Kontextvektor, eine Zusammenfassung der verarbeiteten Sequenz\n",
    "    - Sequenzen liegen als Zahlen vor und das Modell soll aus ihnen lernen\n",
    "    - Insbesondere muss das Modell die \"Semantik\" lernen\n",
    "        - Große Schwierigkeiten (!)\n",
    "        - Ferne Zusammenhänge: **Der Mann**, der an der Haltestelle steht, **ist** mein Onkel --> Zwischen Subjekt und Verb stehen 5 Wörter! Für Menschen ist das easy, für Maschinen etwas weniger\n",
    "\n",
    "### Das Geheimnis ist das Gedächtnis --> Rekurrente neuronale Netze (RNNs)\n",
    "\n",
    "- Informationen \"persistieren\"\n",
    "- Eingaben/Ausgaben variabler Länge verarbeiten\n",
    "- Parameter werden durch die ganze Sequenz geteilt --> Sequenz (z.B. maximale Satzlänge in einer Batch) == Schritte == Zeit\n",
    "    - Parameter:\n",
    "        - U --> unter Eingaben und hidden layers geteilt\n",
    "        - W --> unter den hidden layers geteilt\n",
    "        - V --> unter hidden layers und Ausgaben geteilt\n",
    "\n",
    "<img src=\"documentation/rnn.jpg\" alt=\"Seq2Seq Models\" width=600>\n",
    "\n",
    "Quelle: http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
    "\n",
    "Bezeichnungen:\n",
    "- $x_{t}$ - Input beim Schritt $t$, z.B. das zweite Wort\n",
    "- $s_{t}$ ist der Hidden-State zum Schritt $t$. Das ist der Speicher vom ganzen Netz: $s_{t} = \\text{f}(Ux_t + Ws_{t-1})$ (f = tanh oder ReLU)\n",
    "- $o_{t}$ ist der Ausgabe == Wahrscheinlichkeitsverteilung über alle möglichen Klassen, daher: $o_{t} = softmax(Vs_{t})$\n",
    "\n",
    "---> \"Short Memory\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMs (Long Short-Term Memory)\n",
    "\n",
    "- Hidden State (h oder s) + Cell State (c) (Memory)\n",
    "\n",
    "<img src=\"documentation/LSTM3-chain.png\" alt=\"Seq2Seq Models\" width=600>\n",
    "Quelle: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "Jede Zelle im Bild ist ein Neuron im ganzen Netz.\n",
    "\n",
    "**Gating-Mechanismus**:\n",
    "\n",
    "<img src=\"documentation/lstm_medium.png\" alt=\"Seq2Seq Models\" height=500 width=650>\n",
    "Quelle: https://medium.com/@saurabh.rathor092/simple-rnn-vs-gru-vs-lstm-difference-lies-in-more-flexible-control-5f33e07b1e57\n",
    "\n",
    "- **Forget**-Gate --> Wie viel vom \"Gedächtnis\" muss behalten werden? $f_t$\n",
    "- **Input**-Gate -->  $ i_t$\n",
    "- Berechnung der C-Kandidaten: $\\tilde C_{t}$\n",
    "- **Output-Gate**: $o_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "1. Vocabularies erstellen\n",
    "2. Wörter darstellen --> Embeddings\n",
    "3. Batch generieren --> (seq_len, batch_size)\n",
    "4. Training:\n",
    "    - Batch wird vom Encoder (als Batch) verarbeitet. Encoder liefert h\n",
    "    - Decoder wird durch SOS initialisiert + Target-Batch + encoder hidden state (als erstes hidden state)\n",
    "    - Bei jedem Schritt wird ein Wort vorhergesagt + hidden state aktualisiert\n",
    "5. Die Übersetzung besteht aus den Wörtern mit der höchsten Wahrscheinlichkeit \n",
    "\n",
    "#### Einige Maßnahmen:\n",
    "- Backpropagation Through Time --> Rekursion wird berücksichtigt, daher \"Through Time\"\n",
    "- Teacher Forcing gegen langsame Konvergenz bzw. schlechte Leistungen\n",
    "- Gradient Clipping --> kein Vanishing oder Exploding Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data cleaning\n",
    "start_root = \".\"\n",
    "exp_contraction = True # don't --> do not\n",
    "file_to_load = \"simple_dataset_praesi.txt\"\n",
    "file_name = \"simple_dataset_praesi.pkl\"\n",
    "\n",
    "\n",
    "if os.path.isfile(os.path.join(start_root, PREPRO_DIR,file_name)):\n",
    "    ##load\n",
    "    print(\"File exists. Loading cleaned pairs...\")\n",
    "    pairs = load_cleaned_data(PREPRO_DIR, filename=file_name)\n",
    "else: \n",
    "    print(\"Preprocessing file...\")\n",
    "    ### read lines from file\n",
    "    pairs = read_lines(os.path.join(start_root,DATA_DIR),file_to_load)\n",
    "    ### Preprocess file\n",
    "    pairs, path = preprocess_pipeline(pairs, file_name, exp_contraction, max_len = 0)\n",
    "    \n",
    "print(random.choice(pairs))\n",
    "print(random.choice(pairs))\n",
    "print(random.choice(pairs))\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = pairs\n",
    "src_sents = [pair[0] for pair in pairs]\n",
    "trg_sents = [pair[1] for pair in pairs]\n",
    "\n",
    "max_src_l = max_length(src_sents)\n",
    "max_trg_l = max_length(trg_sents)\n",
    "\n",
    "print(\"Max length in source sentences:\", max_src_l)\n",
    "print(\"Max length in target sentences:\", max_trg_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabularies Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating vocabularies\n",
    "input_lang = build_vocab(src_sents, \"eng\")\n",
    "output_lang = build_vocab(trg_sents, \"deu\")\n",
    "\n",
    "print(\"Total source words:\", input_lang.num_words)\n",
    "print(\"Total target words:\", output_lang.num_words)\n",
    "print(\"\")\n",
    "print(input_lang.word2index)\n",
    "print(\"\")\n",
    "#print(output_lang.word2index)\n",
    "#print(\"\")\n",
    "print(\"Example of conversion word > index:\")\n",
    "print(\"Word {} > Index {}\".format('hello', input_lang.word2index.get('hello')))\n",
    "print(\"Index {} > Word {}\".format(20, input_lang.index2word.get(20)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batching Example\n",
    "\n",
    "<img src=\"documentation/seq2seq_batches.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simple conversion sentence to tensor:\n",
    "random_pair = train_pairs[40]\n",
    "print(random_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sent = indexesFromSentence(input_lang, random_pair[0])\n",
    "german_sent = indexesFromSentence(output_lang, random_pair[1])\n",
    "\n",
    "print(english_sent)\n",
    "print(german_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### No splitting for this short presentation :-)\n",
    "train_pairs = pairs\n",
    "mini_batch = 5\n",
    "batch_pair = [random.choice(train_pairs) for _ in range(5)]\n",
    "test_batch = batch_pair\n",
    "test_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "for pair in test_batch:\n",
    "    print(\"Source:\", pair[0],\"Target:\", pair[1])    \n",
    "    print(\"Src tensor:\", indexesFromSentence(input_lang, pair[0]),\"Trg tensor:\", indexesFromSentence(output_lang, pair[1]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a simple batch of 5 sentences --> Shape (seq_len, batch_size)\n",
    "training_batch = batch2TrainData(input_lang, output_lang, batch_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, input_lengths, target_tensor, mask, target_max_len, target_lengths = training_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of source sentences:\", input_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tensorized input:\")\n",
    "print(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tensorized output:\")\n",
    "print(target_tensor)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Das sieht der Encoder...\n",
    "for i, elem in enumerate(input_tensor):\n",
    "    print(\"Timestep:\", i)\n",
    "    print(\"Input:\", elem)\n",
    "    print(\"Woerter:\", [input_lang.index2word[word.item()] for word in elem])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quellen\n",
    "\n",
    "### Praxis: \n",
    "\n",
    "- Chatbot-Tutorial (PyTorch): https://pytorch.org/tutorials/beginner/chatbot_tutorial.html\n",
    "- Sequence-to-Sequence Tutorial (PyTorch): https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "- Machine Learning Mastery \"How to develop a neural machine translator from scratch\": https://machinelearningmastery.com/develop-neural-machine-translation-system-keras/\n",
    "\n",
    "### Theorie (eine Auswahl):\n",
    "- Sutskever et al. (2014), \"Sequence to Sequence Learning with Neural Networks\": https://arxiv.org/abs/1409.3215\n",
    "- Stanford NLP (CS224N): http://web.stanford.edu/class/cs224n/index.html#schedule\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
